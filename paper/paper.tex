\documentclass[twocolumn]{article}
\usepackage{times}
\usepackage{parskip}
\begin{document}
\title{Cyclone: Fault Tolerance Middleware for NVM Clusters}
\author{Amitabha Roy et al. \\ amitabha.roy@intel.com}
\maketitle
\begin{abstract}
Memory technology in the datacenter is due to undergo a paradigm shift with the
introduction of directly addressable non-volatile memory both in the form of
battery backed non-volatile DIMMs as well as newer memory types such as
3D XPoint. Datacenter applications typically use storage devices to survive
faults that take one more machines offline by combining the persistence offered by
durable storage with availability through replication. Unfortunately, building
fault tolerant applications is known to be hard - users must reason about and
deal with faults by adding fault recovery to their applications. In addition,
replication comes with performance overheads that current research contends can
only be overcome with sophisticated network hardware and intrusive application
changes such as RDMA.

This paper presents Cyclone - fault tolerance middleware for adding reliability
via replication to applications written for NVM. Cyclone aims for API
transparency - replicating RPC calls across replicas for client-server programs
to cause minimum disruption to NVM programming models. In addition, Cyclone
provides at most once RPC execution across the set of replicas with the
  ability to recall the result of the last executed RPC call. As we demonstrate
in the paper - this is sufficient to execute distributed transactions without
the need for users to explicitly handle fault recovery. Cyclone also includes a
high performance but light weight zero copy batching implementation of the RAFT
consensus protocol built around the DPDK network stack. This allows a single
machine to efficiently support multiple RAFT logs at the same time, making full
use of available network resources, replicating tens of millions of RPC calls a
second. This allows Cyclone to provide transparent performance - that is -
minimal performance overheads for concurrent applications written for NVM that
also wish to use Cyclone for fault tolerance without limiting themselves to
distributed transactions on RDMA capable network hardware.
\end{abstract}  

\section{Introduction}
TBD
\section{Background: Single Node Programming Model}
Cyclone builds on top of Intel's Non Volatile Memory Library (NVML) and provides
transparent replication to programs written to use NVML for persistence on a
single node. We describe NVML's programming model in this section to provide
adequate background for the rest of this paper.

NVML provides special functions to allocate and free persistent memory as well
as fat pointers to persistent memory to ensure that referential integrity in the
heap is maintained across restarts - since the virtual memory ranges into which
persistent memory is mapped may change.

\begin{figure}
{\scriptsize
\begin{verbatim}

struct ll_node {
  int value;
  TOID(ll_node) next;
};

void insert_after(TOID(ll_node) prev, 
                  TOID(ll_node) new_node)
{
  D_RW(new_node)->next = D_RO(prev)->next;
  D_RW(prev)->next = new_node;
}

\end{verbatim}
}
\caption{A Persistent Linked List of Integers}
\label{fig:example}
\end{figure}

Figure~\ref{fig:example} shows the example of a linked list node and a function
for inserting a new node after a given one. The {\tt TOID} declaration wraps the
linked list node type to produce a fat pointer to it. A {\tt TOID(ll\_node)}
type therefore is a pointer to a linked list node stored in persistent memory.
The {\tt insert\_after} function uses the {\tt D\_RO} and {\tt D\_RW} calls to
convert fat pointers into directly accessible read-only and read-write pointers
respectively. Other than the mechanics of using fat pointers the code entirely
resembles that which a programmer might write for a standard linked list in
DRAM.

\subsection{Failure Atomicity}
Programmers typically find it hard to reason about failures when writing
code. This is a problem when directly manipulating data structures in persistent
memory. Depending on the specifics of the system architecture, updates to
persistent memory can be persisted out of order thereby leaving the data
structure in an inconsistent state. To illustrate this consider a failure during
execution of the code to add a node to a linked list in
Figure~\ref{fig:example}. If either of the two updates to the next fields of the
previous and new node fail to be persisted, the linked list is left in a
disconnected state with one or more connected nodes inaccessible.

NVML provides tools for programmers to write code that is failure
atomic by allowing a block of code to be delimited as a transaction. A
transaction is either completely executed in terms of its effects on
persistent memory or not executed at all. Under the hood, NVML tracks all
updates to persistent memory in a persistent undo log - the log entries are
persisted before the update to the actual memory is allowed. On a failure, the
undo log is used to roll back incomplete transactions. A transaction is
considered complete when it completes execution and the updated objects are
flushed (automatically by NVML) to persistent memory. The log entries for the
transaction are then immediately deleted. The only burden placed on a programmer
for failure atomicity is to delimit transactions and that they must inform NVML
about objects that are updated. Figure~\ref{fig:example_fatomic} shows how to
convert the function for adding a node to a linked list into a failure atomic
one. The only additions are the transaction delimiters and the {\tt TX\_ADD}
calls to mark the objects being updated \emph{before} the actual updates to
them.

\begin{figure}
{ \scriptsize
\begin{verbatim}

struct ll_node {
  int value;
  TOID(ll_node) next;
};

void insert_after(TOID(ll_node) prev, TOID(ll_node) new_node)
{
  TX_BEGIN {
    TX_ADD(new_node);
    D_RW(new_node)->next = D_RO(prev)->next;
    TX_ADD(prev);
    D_RW(prev)->next = new_node;
  } TX_END
}

\end{verbatim}
}
\caption{Failure Atomic Peristent Linked List}
\label{fig:example_fatomic}
\end{figure}

A crucial feature of NVML is that it abstracts the details of the persistence
mechanism away from programmers. This allows the same code to be deployed on
across a range of persistence mechanisms - mapping pages from secondary
storage such as SSDs, using battery backed NVDIMMs, using an onboard battery to
save DRAM state to SSD on failure [FARM] or using directly accessed persistent
memory [PMFS]. Each of these options has different requirements. For example the
NVM used in PMFS requires explicit cache flush (CLFLUSH) and PCOMMIT
instructions. NVDIMMs would conceivably require only a CLFLUSH to flush the
cacheline and ensure that the line enters the asynchronous data refresh
domain. Using secondary storage is also an option with faster 3DXPoint storage
behind an NVMe interface - this would require an {\tt msync} and OS pagecache
intervention to persist data to secondary storage. NVML seamlessly uses the
right persistence mechanism without requiring code using the library to be aware
of these details.

\section{Replication}
Cyclone is designed to add fault tolerance to NVML client server applications
via replication - multiple machines maintain \emph{equivalent} state. Server
state is queried and manipulated via RPC calls from clients. Cyclone's approach
to replication is simple - we replicate the RPC call itself across replicas
rather than replicating every access made during execution of the RPC call,
which would be prohibitively expensive.

Cyclone provides strongly consistent replication - every machines in the quorum of
replicas maintains a log of RPC calls and all machines agree on the sequnce of
RPC calls in the log. We achieve this by running an instance of the RAFT [raft]
consensus protocol to keep the log of RPC calls on different machines in
synchronization. The standard replicated state machine approach requires that
commands (entries in the log) be committed before they can be applied to the
state machine i.e. in our case - the RPC call be executed. However, the fact
that RPC calls that modify state must themselves be executed in a crash
consistent transaction allows us to overlap execution with replication as we
describe next.

\subsection{Decoupled Replication}
Cyclone decouples execution of the RPC call on any replica from its replication
by performing them in parallel to save time. Normally, this is a problem with
any consenus protocol including RAFT as replication might failt for a number of
reasons primarily due to a change of leader and subsequent of rollback of
previously appended (but not committed) log entries. In order to achieve
decoupled replication therefore Cyclone depends on the ability of NVML to roll
back crash consistent transactions. The ability to roll back a crash consistent
transaction is important to tolerate power failure as incomplete transactions
must be rolled back after recovery from a power failure to preserve the
integrity of the heap. NVML also allows the user to roll back a transaction if
needed. Cyclone exploits this feature by delaying the commit of the failure
atomic transaction after execution till replication is complete. In the event
that replication fails, the transaction is aborted and any changes to the non
volatile heap are rolled back.

A pseudocode description of how the runtime handles decoupled replication is
shown in Figure~\ref{fig:async_rep}.

\begin{figure}
{ \scriptsize
\begin{verbatim}
Initiate replication of RPC call and arguments
TX_BEGIN {
  Execute RPC call and modify local NVM
  Block till replication result known
  if(replication failed) {
    TX_ABORT;
  }
} TX_END
\end{verbatim}
}
\caption{Asynchronous Replication}
\label{fig:async_rep}
\end{figure}

Asynchronous replication provides good performance and we believe would be the
common case for RPC calls replicated via Cyclone. However the program might on
occasion require successful replication to be a precondition to exection. A case
where we find this necessary is output commit - where the RPC call has external
side effects, such as communicating with other machines in the cluster and
therefore crash consistency is insufficient to roll back the execution of the
RPC call in the event that replication fails. We therefore provide users the
ability to declare at the client side that an RPC call should execute
synchronously at the server side. This results in Cyclone ensuring that the RPC
call is successfully replicated before it attempts execution on any of the
replicas.

\subsection{Zero Copy Batching}
Replication protocols are expensive to operate - fact that has been underlined in
multiple pieces of research [FARM, in a box]. It would therefore appear that
replication would become the bottleneck in Cyclone. We observed however that
previous work did not implement replication around a high performance network
stack possibly explaining why they chose both high performance hardware (40
GigE+ NICS, custom FPGA designs) as well as specific high performance but
restrictive software APIs (one sided RDMA with metadata co-location, application
compiled down to the FPGA). As a counterpoint, we designed Cyclone around DPDK
[dpdk] a high performance user-space network stack. DPDK provides convenient
access to packets arriving at an ethernet NIC by moving them into packet buffers
with the contained cache lines also directly placed into the last level cache of
the CPU during DMA, a first step to a high performance implementation of RAFT.

A second step is the observation that replication is by definition a zero copy
activity. The exact same RPC call must be sent to all replicas - the protocol
state machine by itself has no role to play in the contents of the log
entries. We therefore designed our implementation of RAFT in Cyclone to avoid
copying incoming packets containing RPC calls from clients when replicating
them. The only addition from the protocol is a RAFT specific header containing
the current term and index of the RPC call in the RAFT log and term of the
leader, which is prepended to the packet before sending it out for replication.

The third and final step is batching. Although simple, RAFT's consensus protocol
requires a non-zero number of CPU cycles to process each entry in log. To
amortize this cost, we make extensive use of batching. DPDK by itself can
efficiently batch incoming packets under load - allowing the user to easily
receive a burst (currently limited to 32) packets rather than a single one for a
polling call. DPDKs separation of packet data from metadata means that we can
chain these packets together and treat them as a single log entry for the
purposes of replication.

TBD - put it together diagram.

\subsection{Multiple RAFT Logs}
Say we maintain multiple RAFT logs on the same machine with dedicated NIC queue pairs.

\subsection{RPC Call Execution}
Call steering
Exactly once


\section{Putting it Together - A linearizable hash table}
\label{sec:examples}
We now demonstrate how Cyclone can be used in practise by putting together an
example of a linearizable set of counters. The problem is to construct a sparse
set of counters indexed by a 64 bit key and support \emph{linearizable}
increment, decrement and an atomic set increment. The atomic set increment takes
as input a set of counter indexes and their current value and atomically
increments the counters from the the given values. 

Figure~\ref{fig:counter_set} shows a linearizable implementation executing on
servers. Odd counter values are considered unstable and even values stable.
Clients make RPC calls to servers the trigger the execution of the
appropriate function. Any client can make calls for the get and increment
operations shown on the left hand side. A client retries an operation until it
finds a stable counter value.

A single distinguished client called the co-ordinator runs any atomic increment
set operations on behalf of other clients - shown on the right hand side of
Figure~\ref{fig:counter_set}. We chose to have a single
distinguished co-ordinator to avoid making the example complex by introducting
concurrency.

The operations in Figure~\ref{fig:counter_set} do not comprehend failures.
Cyclone includes a client library to automatically sequence RPC calls at the client
side, retry them, connect to a new quorum leader on a view change and finally
take care of unrealiable networks. At the server side the results of increment,
prepare and commit RPC calls are always persisted and replicated before the
response to the client. For individual get, increment, prepare and commit
operations therefore persistence, replication and failover is transparently
handled.

The AtomicIncrementSet operation presents a special case however. Clients can
fail and therefore a client executing the steps of the operation can fail midway
leaving some counters in an unstable state forever. The solution is the
replicate the distinguished co-ordinator client too. The co-ordinator therefore
becomes both a server, receiving AtomicIncrementSet operations from clients as
well as a client, making individual Prepare and Commit calls to the quorum
leader. The implementation of the AtomicIncrementSet however requires guarantees
beyond that of simple asynchronous replication

\begin{itemize}
  \item Output Commit: AtomicIncrementSet has externally visible side effects at
    other servers when execting. It should therefore be guaranteed to finish
    executing once it starts
  \item Exactly once: Regardless of restarts AtomicIncrementSet should be
    preceived to have executed exactly once by an external observer.
\end{itemize}

The solution is to simply run AtomicIncrementSet in synchronous replication
mode. This guarantees the first property. It also guarantees that exactly once
execution will succced in ``leader'' mode with followers simply remembering the
success status of a transaction. To ensure that due to a failover we do not
prepare or commit a counter more than once, we use exactly once RPCs that
prevent RPC calls from executing more than once. Finally, since only
one AtomicIncrementSet is executed at a time, we include the current step (the
place in the for loop) in RPC calls to the quorum leader and have it reflect the
information back in the response (a cookie of sorts). By querying the results of
the last made call, the co-ordinator can simply fast forward to the correct
index in the for loop during recovery and use the value of the ``success'' field
in the response to update its own local success variable.


\begin{figure*}
\begin{tabular}{cc}    
\begin{minipage}{0.5\textwidth}  
{ \scriptsize
\begin{verbatim}
bool is_stable(uint64_t value)
{
  if(value is even) {
    return true
  }
  else {
    return false
  }
}

Get(uint64_t index)
{
  if(not is_stable(counters[index]) {
    return EUNSTABLE
  }
  else {
    return counters[index]
  }
}

Increment(uint64_t index)
{
  if(not is_stable(counters[index]) {
    return EUNSTABLE
  }
  else {
    counters[index] = counters[index] + 2
    return SUCCESS
  }
}
\end{verbatim}
}
\end{minipage} &
\begin{minipage}{0.5\textwidth}  
{ \scriptsize
\begin{verbatim}
Prepare(uint64_t index, 
        uint64_t value, 
        bool success)
{
  if(counters[index] == value) {
    success = success AND TRUE
  }
  else {
    success = success AND FALSE
  }
  // Make unstable
  counters[index] = counters[index] + 1
}

Commit(uint64_t index, bool success)
{
  if(success == TRUE) {
    // Update
    counters[index] = counters[index] + 1
  }
  else {
    // Rollback
    counters[index] = counters[index] - 1
  }
}

AtomicIncrementSet(uint64_t indices[],
                   uint64_t pre_values[],
                   int count)
{
  bool success = TRUE
  for i = 1 to count
    success =
      success AND
        Prepare(indices[i],
                pre_values[i],
                success)
  for i = 1 to count
    Commit(indices[i], success)       
  return success
}
\end{verbatim}
}
\end{minipage}
\end{tabular}
\caption{Counter Set}
\label{fig:counter_set}
\end{figure*}

\section{Related Work}

\end{document}




\documentclass[twocolumn]{article}
\usepackage{times}

\begin{document}
\title{Cyclone: Fault Tolerance Middleware for NVM Clusters}
\author{Amitabha Roy et al. \\ amitabha.roy@intel.com}
\maketitle
\begin{abstract}
Memory technology in the datacenter is due to undergo a paradigm shift with the
introduction of directly addressable non-volatile memory both in the form of
battery backed non-volatile DIMMs as well as newer memory types such as
3D XPoint. Datacenter applications typically use storage devices to survive
faults that take one more machines offline by combining the persistence offered by
durable storage with availability through replication. Building such fault
tolerant applications is hard and users are forced to either build
them from scratch or modify their applications to fit the interfaces of solutions
such as zookeeper. In addition to a loss of flexibility such applications today
are unable to take advantage of the speed of directly attached non-volatile
memory as opposed to older slower storage in the form of disks and SSDs.

This paper presents Cyclone - fault tolerance middleware that exposes a
replicated and durable heap. This heap can be directly manipulated using loads and
stores. Cyclone transparently provides both failure atomicity as well as
strongly consistent replication thereby giving strong guarantees to programmers
with minimal effort on their part. Under the hood Cyclone decouples modification
of local persistent memory from replication to maximize performance. In
addition, unlike traditional systems that provide strong replication, Cyclone
does not take checkpoints since NVM is directly manipulated by programmers and
therefore up to date till the last executed transaction. Instead, Cyclone
provides a novel NVM copy mechanism to bring up new nodes added to the cluster.
\end{abstract}  

\section{Introduction}

\section{Single Node Programming Model}
Cyclone provides users with a replicated and durable heap that can be
manipulated via a set of library routines. The library provides special
functions to allocate and free persistent memory as well as fat pointers that
wrap pointers to persistent memory to ensure that they do not change across
restarts - since the virtual memory ranges into which persistent memory is
mapped may change.

\begin{figure}
{ \scriptsize
\begin{verbatim}

struct ll_node {
  int value;
  TOID(ll_node) next;
};

void insert_after(TOID(ll_node) prev, TOID(ll_node) new_node)
{
  D_RW(new_node)->next = D_RO(prev)->next;
  D_RW(prev)->next = new_node;
}

\end{verbatim}
}
\caption{A Persistent Linked List of Integers}
\label{fig:example}
\end{figure}

Figure~\ref{fig:example} shows the example of a linked list node and a function
for inserting a new node after a given one. The {\tt TOID} declaration wraps the
linked list node type to produce a fat pointer to it. A {\tt TOID(ll\_node)}
type therefore is a pointer to a linked list node stored in persistent memory.
The {\tt insert\_after} function used the {\tt D\_RO} and {\tt D\_RW} calls to
convert fat pointers into directly accessible read-only and read-write pointers
respectively. Other than the mechanics of using fat pointers the code entirely
resembles that which a programmer might write for a standard linked list in
DRAM.

\section{Failure Atomicity}
Programmers typically find it hard to reason about failures when writing
code. This is a problem when directly manipulating data structures in persistent
memory. Depending on the specifics of the system architecture, updates to
persistent memory can be persisted out of order thereby leaving  the data
structure in an inconsistent state. To illustrate this consider a failure during
execution of the code to add a node to a linked list in
Figure~\ref{fig:example}. If either of the two updates to the next fields of the
previous or new node fail to be persisted, the linked list is left in a
disconnected state with one or more connected nodes inaccessible.

Cyclone provides tools for programmers to write code that is failure
atomic. Portions of the code are delimited as transactions that are either
completely executed in terms of its effects on persistent memory or not executed
at all. Under the hood, Cyclone tracks all updates to persistent memory in a
persistent undo log - the log entries are persisted before the update to the
actual memory is allowed. On a failure, the undo log is used to roll back
incomplete transactions. A transaction is considered complete when it completes
execution and the updated objects are flushed (automatically by Cyclone) to
persistent memory. The log entries for the transaction are then immediately
deleted. The only burden placed on a programmer for failure atomicity is that
they must inform the Cyclone runtime about objects that are
updated. Figure~\ref{fig:example_fatomic} shows how to convert the function for
adding a node to a linked list into a failure atomic one. The only additions are
the transaction delimiters and the {\tt TX\_ADD} calls to mark the objects being
updated \emph{before} the actual updates to them. Figure~\ref{fig:undo_log}
shows how Cyclone maintains the undo-log of objects.


\begin{figure}
{ \scriptsize
\begin{verbatim}

struct ll_node {
  int value;
  TOID(ll_node) next;
};

void insert_after(TOID(ll_node) prev, TOID(ll_node) new_node)
{
  TX_BEGIN {
    TX_ADD(new_node);
    D_RW(new_node)->next = D_RO(prev)->next;
    TX_ADD(prev);
    D_RW(prev)->next = new_node;
  } TX_END
}

\end{verbatim}
}
\caption{A Persistent Linked List of Integers}
\label{fig:example_fatomic}
\end{figure}

\begin{figure}
{ \scriptsize
\begin{verbatim}
TX_BEGIN()
{
  Initialize undo log
}

TX_ADD(o)
{
  if(o not already in undo log) {
    Append <o, value of o> to undo log
  }
}

TX_END()
{
  for each entry <o, v> in undo log {
    flush o to persistent storage
  }
  truncate undo log
}

Recover()
{
  for each entry <o, v> in undo log {
    set o to v
    flush o to persistent storage
  }   
}

\end{verbatim}
}
\caption{Undo Log in Cyclone}
\label{fig:undo_log}
\end{figure}

A crucial feature of Cyclone is that it abstracts the details of the persistent
mechanism away from programmers. This allows the same code to be deployed on
clusters with a persistent storage medium such as 3D XPoint or on clusters with
battery backed NVDIMMs for example. In the former case, explicit {\tt PCOMMIT}
instructions are necessary for persistence [ref PMFS paper], while in the latter
case no such instructions are necessary as on failure the CPU caches and DRAM
are flushed to stable storage under battery (or capacitor) power. Even in the
NVDIMM case however, the undo log is necessary to protect against OS or
application crashes to ensure that the persistent memory is always left in a
consistent state.


\section{Replication}
Cyclone replicates the state of the NVM heap across multiple machines in a
cluster. Cyclone provides strong consistency - a majority of machines in the
quorum of replicas always agrees on the current state of the NVM heap. Unlike
traditional approaches to strong replication however, Cyclone is designed to
decouple replication from execution of user code modifying NVM. It does so by
encapsulating changes in to the NVM heap in RPC calls driven by client requests.
We chose this model since it closely reflects the use cases most suitable for
NVM-based clusters.

Cyclone replicates state \emph{by executing the same sequence of RPC calls on
  all members of the quorum}. Unlike traditional replication therefore Cyclone
does not replicate logs of changes to apply to replicas. Instead it replicates
the RPC call and its arguments, executing it on every replica. By executing the
same sequence of RPC calls on all machines we end up with the same
state. Cyclone uses RAFT under the hood for consensus on the contents of the log
of RPC calls - although any other similar state machine replication protocol
such as Paxos is also suitable. Cyclone provides the means to achieve exactly
once semantics to clients by numbering RPC calls and remembering the results of
the last executed RPC call.

\subsection{RPC Call Numbering}
Cyclone is preconfigured for a fixed (large) number of clients. All RPC calls
from a client are numbered and Cyclone remembers the results of the last RPC
call made by a client. A particular client can exist on any machine - cyclone
treats all instances of the client as a single client. Cyclone rejects RPC calls
that are not in sequence. Any RPC call is executed exactly once across the
quorum - regardless of failure and reconfigurations. This provides the means for
clients to easily achieve exactly once semantics, provided the client does not
crash. Cyclone can supply the last executed RPC call number and its result to
assist recovering and stateless clients.

\subsection{Asynchronous Replication}
By default, Cyclone decouples execution of the RPC call on any replica from its
replication. This is particularly beneficial to performance since it allows
overlapping the update of non-volatile state with the network I/O associated
with replication. Persisting RAFT log entries, making the upcall to the
application and executing the RPC call in a failure atomic transaction takes a
significant amount of time and therefore it makes sense to overlap this with the
round-trip delay on the network for replication. This is possible in Cyclone and
not other traditional replication systems since we use failure atomic
transactions that cna be rolled back. Replication can fail due to various
reasons such as failure of nodes, a network partition or a new leader getting
elected in the underlying replication protocol (RAFT). Since failure atomic
transactions can be rolled back, we are able to speculatively apply the changes
to NVM without waiting for replication to finish.

The runtime steps associated with asynchronous replication are shown in
Figure~\ref{fig:async_rep}.

\begin{figure}
{ \scriptsize
\begin{verbatim}
Initiate replication of RPC call and arguments
TX_BEGIN {
  Execute RPC call and modify local NVM
  while(!replication_complete);
  if(replication_failed) {
    TX_ABORT;
  }
} TX_END
\end{verbatim}
}
\caption{Asynchronous Replication}
\label{fig:async_rep}
\end{figure}

\subsection{Synchronous Replication}
Asynchronous replication provides good performance but lacks some guarantees
necessary for certain distributed systems paradigms such as output commit and
the ability to integrate code that has non-deterministic execution paths or side
effects - such as reading the time. Cyclone therefore provides programmers the
ability to closely synchronize execution across the replicas using synchronous
replication. Synchronous replication extends the programming model provided by
cyclone to users - neccessitating some awareness on the part of the user about
the difference between the leader and follower replicas. We show later
(Sec~\ref{sec:examples}) how this awareness enables the crafting of efficient
fault tolerant application with little effort on the part of the programmer.

Figure~\ref{fig:sync_rep} shows how synchronous replication operates. Execution
of the RPC call is only initiated when the call and its arguments have been
replicated. This ensures that any side effects such as calls to other machines
only happen after it is guaranteed that the effects of the RPC call will be
committed to the replicas. This provides the output commit property necessary
for some of the applications we discuss below. Next the RPC call is executed on
the leader and the result of the RPC call is replicated to the followers. The
followers then proceed to execute the RPC call locally taking into account the
result from the leader. This result can include data to synchronize
non-deterministic steps such as reading the time on the replica. This process is
always conditional on the view (current configuration of replicas and choice
leader) staying unchanged. On a view change the process is reexcuted to ensure
than only one set of RPC call results is propagated across the replicas. 

\begin{figure*}
{ \scriptsize
\begin{verbatim}
Initiate replication of RPC call and arguments
TX_BEGIN {
  while(!replication_complete);
  if(replication_failed) {
    TX_ABORT;
  }  
  while(true) {
    if(is_leader && !have_execution_result) {
      Execute RPC call and modify local NVM
      replicate_rpc_call_result in current view;
      if(result replication failed) {
        continue;
      }
      while(!result replication complete && !view change);
      if(view change) {
        continue;
      }
    }
    else {
      while(!result replication complete && !view change);
      if(view change) {
        continue;
      }
      Execute RPC call to modify local NVM with result from leader
    }
  }
} TX_END
\end{verbatim}
}
\caption{Synchronous Replication}
\label{fig:sync_rep}
\end{figure*}


\section{Example Applications}
\label{sec:examples}

\section{Evauation}

\subsection{Baseline}

\subsection{Configuration Management}

\subsection{Transaction Processing}

\subsection{Network Function Virtualization}

\section{Related Work}

\end{document}




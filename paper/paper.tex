\documentclass[pageno]{jpaper}

%replace XXX with the submission number you are given from the ASPLOS submission site.
\newcommand{\asplossubmissionnumber}{XXX}

\usepackage[normalem]{ulem}
\usepackage{parskip}

\begin{document}

\title{\Large \bf Cyclone: Warp Speed Replication for Key Value Stores}


\date{}
\maketitle

\thispagestyle{empty}

\begin{abstract}
Persistent key value stores are rapidly becoming an important component of many
distributed data serving solutions with innovations targeted at taking advantage
of growing flash speeds. Unfortunately their performance is hampered by the need
to maintain and replicate a write ahead log to guarantee availability in the
face of machine and storage failures. Cyclone is a replicated log plug-in for
key value stores that systematically addresses various sources of this
bottleneck. It uses a small amount of non-volatile memory directly addressable
by the CPU - such as in the form of NVDIMMs - to remove block oriented IO
devices such as SSDs from the critical path for appending to the log. It then
addresses network overheads using an implementation of the RAFT consensus
protocol that is designed around a userspace network stack to relieve the CPU of
the burden of data copies. Finally, it provides a way to efficiently map the
commutativity in key-value store APIs to the parallelism available in commodity
NICs via multiple transmit receive queues to scale replication performance on
multicore machines. Cyclone is able to replicate millions of small updates a
second using only commodity 10 gigabit ethernet adapters. As a practical
application, we use it to improve the performance and availability of RocksDB, a
popular persistent key value store by an order of magnitude when compared to its
own write ahead log without replication.
\end{abstract}

\section{Introduction}
Persistent key value stores are an increasingly important component of
datacenter scale storage services. Key value stores such as
Rocksdb~\cite{rocksdb}, LevelDB~\cite{leveldb} and FloDB~\cite{flodb} represent
large amounts of effort on both the engineering and research fronts. These key
value stores include sophisticated in-memory data structures built around Log
Structured Merge trees~\cite{lsmtree} and are heavily tuned to extract maximum
performance from flash-based solid state drives (SSDs).

These key value stores however tend to ignore an important component: the write
ahead log. A machine or storage failure leading to total loss or temporary
unavailability of data is unacceptable in services where high availabilty and
revenue are interconnected. Key value stores therefore usually incorporate
support for a write ahead log that if replicated and kept durable for every
appended update provides the necessary high availability. Unfortunately, the
write ahead log is a performance achilles heel for these systems - eclipsing
much of the work on improving the performance of the LSM component. To
illustrate the impact of the write ahead log, consider
Figure~\ref{fig:problem}. The line marked 'Rocksdb' shows the performance of
Rocksdb without the write ahead log. The performance when persisting the write
ahead log without replicating it is shown as the line marked 'rocksdb/WAL'. The
line marked 'rocksdb/3 way rep.' is for simply replicating the log three ways
without persisting it, using RAFT~\cite{raft} running over TCP/IP. Either
persisting every update to the log or replicating it using TCP/IP causes
performance to drop by an order of magnitude (note the log scale on the
x-axis). It is therefore no surprise that deployments of Rocksdb often turn off
the write ahead log~\cite{samza}. On the other side of the spectrum, key value
store research prototypes such as FloDB~\cite{flodb} turns off the write ahead
log to be able to showcase benefits of sophisticated extensions to LSM
datastructures.


\begin{figure}
\centering \includegraphics[scale=0.6]{results2/problem.pdf}
\caption{Rocksdb write ahead logging impact}
\label{fig:problem}
\end{figure}


Cyclone is a high speed strongly consistent replicated write ahead logging
service specialized for key value stores such as Rocksdb. We describe Cyclone's
system architecture in Section~\ref{sec:sysarch}. Cyclone provides the order of
magnitude better performance required to close the performance gap in
Figure~\ref{fig:problem} both on the storage and network side. Cyclone requires
\emph{a small amount of} directly attached Non Volatile Memory (NVM) on the host
- such as in the form of NVDIMMs~\cite{farm} or directly attached persistent
memory~\cite{pmfs}. Cyclone logs update operations to the key value store and
calls on the key value store to execute the update only when it has been
successfully replicated to majority quorum of replicas. All reads and write to
the key value store are always executed by the leader replica. A client side
stub provided by Cyclone automatically locates and directs read and write
requests to the leader replica, ensuring linearizable semantics from the key
value store. The novelty in Cyclone is that it accomplishes this without
compromising on the performance baseline of no logging or replication. Cyclone
does this by systematically addressing the sources of performance degradation in
Figure~\ref{fig:problem}.

The primary cause of slowdown when turning on a write ahead log on a single
machine is the need to do synchronous block IO for every update. For even
moderately sized updates, this leads to severe write amplification as a 4KB page
must be synchronously written to the log file on flash. Cyclone avoids this by
making use of directly attached persistent memory on the server. This can be in
the form of NVDIMMs (DIMMs with an added ultracapacitor to dump state to a small
amount of attached flash) or newer forms of persistent memory~\cite{pmfs}. For
current deployments, NVDIMMs are expensive - retailing at approximately 2X the
price of a standard DIMM [find ref]. This means that they will likely be
available in small quantities. We compensate for this by periodically draining
the NVDIMM log into a log placed on a standard SSD on the system. We describe
this two level log structure in Section~\ref{sec:storage}.

The network overheads for replicating a log entry to a quorum of machines stems
from the overhead of the TCP/IP stack in the linux kernel - itself the topic of
much research [dataplane OS stuff]. We therefore replase the TCP/IP stack with
DPDK, a userspace network library that allows direct IO of raw ethernet frames
to and from the network interface card (NIC) eliminating all overheads. We no
longer use TCP - since the rare network errors encountered in a typical
datacenter network are automatically handled by the replication protocol (RAFT)
as part of its failure model. Finally, we carefully design our implementation of
RAFT to relieve the CPU of all data copy operations, leaving the CPU to only
run the protocol and return an accept/reject decision for each received
packet. The set of optimizations to the network stack is decribed in
Section~\ref{sec:network}.

Both the storage (NVM, SSD) as well as the NIC expose significant parallelism
that is left unused when Cyclone is used to replicate a single log. To exploit
this parallelism we use Cyclone to replicate multiple logs, each using a
different instance of the RAFT consensus protocol. We show how to exploit the
commutativity inherent in key-value store interfaces to make use of this
parallelism in Cyclone's design in Section~\ref{sec:parallelism}.

A detailed evaluation of Cyclone's replication is provided in the context of
RocksDB - a popular persistent key value store - in
Section~\ref{sec:evaluation}. We show that Cyclone achieves performance
transparent replication in that it can provide both a persistent write ahead log
and replication without compromising on RocksDB's performance.

\section{System Architecture}
\label{sec:sysarch}
A persistent key value store durably stores key-value mappings accessible through a
simple interface:

\begin{itemize}
\item GET(K): returns value corresponding to key K
\item PUT(K, V): sets the value of key K to value V
\end{itemize}

A key value store typically serves multiple client requests concurrently and is
linearizable~\cite{linearizability}. This means any history of requests and
responses can be rearranged such that responses immediately follow corresponding
requests and if a response precedes a request in the original history it also
does so in the rearranged history. Key value stores such as RocksDB also
support atomic writes to multiple keys and snapshots. 

Cyclone integrates with key value stores as both a client and server side
library. Cyclone replicates the key value store across a set of replica
servers. The client side library sends requests to a distinguished leader
replica.  On the server side Cyclone accepts requests from clients and calls
into the key value store. It then returns a response to the client. Before
executing any PUT request, Cyclone appends it to a durable log and replicates
the request to the logs of follower replicas. The request is considered
replicated once acknowledged by a majority of replicas.

Cyclone uses RAFT~\cite{raft} to keep the logs in sync across the replicas. On a
failure of the leader replica a new leader is automatically elected - RAFT
ensures that the new leader has the most up to date logs. The Cyclone client
library automatically locates the new leader on a failover - clients therefore
see only a brief interruption in service. Although read requests (GET
operations) are not logged, they are serviced in FIFO order with the write requests
(PUT operations) currently in Cyclone's log. This ensures linearizability in the
face of failovers.

\section{Storage}
\label{sec:storage}
Storage devices such as flash-based SSDs export a block IO interface. Appending
a small update to the write-ahead log requires a synchronous update of an entire
4KB page. This leads to a high baseline latency at even low load, as shown in
Figure~\ref{fig:problem}.  On the other hand small updates (in the order of 100s
of bytes for both the key and value taken together) are commonplace in many
workloads [find refs]. This problem leads many system designers to
underprovision key-value store shards ensuring that it operates at moderate to
high load, using group-commit to batch additions to the log. They therefore pay
a price in latency to ensure the bandwidth to storage is fully utilized to
prevent it from becoming a bottleneck.

Cyclone adopts a different strategy by making use of novel memory technologies
to obviate the need to make this tradeoff in the first place. We assume a small
amount of non-volatile memory directly addressable by the CPU rather than placed
behind a block oriented IO interface. Log appends are done to the ``NVM log''
placed in this non-volatile memory. While we designed Cyclone to leverage new
non-volatile memory technologies that might be available in the
future~\cite{3dxpoint} current non-volatile memory options such are NVDIMMs
are more expensive that DRAM. Therefore, for Cyclone to be economical
we cannot assume that a sufficient amount of NVM is available to hold the entire
log. The log entries can only be garbage collected when the in-memory component
of the key value store is checkpointed to secondary storage. The growing size of
available DRAM means that also provisioning sufficient NVDIMMs to hold the
corresponding log of updates is not feasible given that NVDIMMs cost at least
twice as much as volatile DRAM. We therefore resort to using a second larger log
level placed on a flash SSD. Entries are drained from the NVM log to this flash
log in conveniently large units that are a multiple of 4KB - the optimum IO unit
for flash SSDs. This ensures we get the best
possible throughput from a flash device without paying the price of synchronous
IO for small key-value pairs. At the same time, the amount of non-volatile
memory required is small enough to not add undue cost of the server.

The flashlog is written out in segments of configurable size (we use 128KB
segments). A segment buffer is prepared in memory (volatile DRAM) using the
layout shown in Figure~\ref{fig:flashlog_page}. We do not allow objects in the
flashlog to cross a 4KB boundary - linking multiple objects together with a
special flag encoded into the size if necessary. We flush log segments out to
the log file using asynchronous direct IO, and therefore we fill one log segment
buffer while keeping IO to another one outstanding to the flash drive. To avoid
having to do a synchronous metadata flush, we preallocate (using the
posix\_fallocate call) a gigabyte worth of zero filled disk pages at the end of
the log file whenever we hit its end.

In order to recover from a crash, we make two important assumptions about the
underlying SSD. First, we assume that 4KB is the \emph{minimum} atomic unit for
updating pages on the SSD even under power failure i.e. there are no shorn
writes (otherwise known as torn writes) on a 4KB page~\cite{shorn_writes}. We
also assume that the SSD has power loss data protection meaning that writes
cached in the drive's volatile cache are written to the SSD using a backup
capacitor in the event of power failure - a property of many data center class
SSDs today, including the Intel DC P3600 SSD~\cite{ssd_spec} we use in our
evaluation. Together these two assumptions mean that we can recover a consistent
prefix of the log on a power failure.

We move log entries from the head of the NVM log to the flashlog buffers in FIFO
order. The NVM log entry is only actually removed when the IO for the
corresponding flashlog page is complete. This means that during recovery we can
have the same log entry both in the NVM log and the flashlog, a condition that
can be detected by examining the log sequence number that we embed in each log
entry.

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{figures2/flashlog_page.pdf}
  \caption{Flashlog segment}
  \label{fig:flashlog_page}
\end{figure}

The two level log design in Cyclone lays the foundation for improvements to the
network stack that we discuss next - a significant contributor to the overheads
in Figure~\ref{fig:problem}. We use RAFT to replicate the NVM log from the
leader replica to the followers. Each replica \emph{independently} drains
committed entries (in terms of RAFT consensus) from the NVM log to the
flashlog. Block IO therefore is no longer a consideration when optimizing data
movement over the network, unlike systems that need to deal with such
problems~\cite{reflex}. Replication reduces to efficiently sending a block of
data already present in directly attached memory over the network. 

\section{Network}
\label{sec:network}

The core network operation in replication is to receive a request from the
client at the leader replica and send that exact same request (as a log entry)
to all follower replicas. The leader therefore \emph{multicasts} a received
request packet to follower replicas, a problem well studied in the networking
community when building software packet switches. Software packet switches reach
impressive speeds of millions of packets forwarded per core [find refs], a
number far in excess of the few hundreds of packets we manage in
Figure~\ref{fig:problem}. We therefore proceed by approximating a software
packet switch for the networking component of Cyclone. However software packet
switching does not need to deal with failure - usually requiring only a simple
decision for each packet at the node. RAFT introduces significant
protocol level work for each forwarded packet a challenge that we need to deal
with to approach the desired software packet switching efficiency in the common
case.

\subsection{DPDK}
The Data Plane Development Kit~\cite{dpdk} provides low latency userspace access
to an ethernet NIC, permitting the application to directly send and receive raw
ethernet frames via the transmit and receive queues on the NIC. DPDK is often
used by developers of software packet switches and therefore we also leverage
DPDK as a library for building Cyclone.

DPDK does not by itself provide a TCP stack. This is not a problem. RAFT (and
indeed most consensus protocols) tolerate network losses and reordering by
design due to the need to support asynchronous communication. In addition, most
datacenter networks rarely drop or reorder packets and provide full bisection
bandwith between servers that might serve as Cyclone replicas. We therefore
jettisoned TCP and chose to send raw IP packets encapsulated in ethernet
frames. We currently follow this communication model both for server to server
communication between replicas as well as client server communication for
requests and responses. Although a detailed evaluation is made later, switching
from the kernel TCP/IP stack to DPDK reduces the latency between machines in our
testbed from {\tt 24 us} to as low as {\tt 7 us}, providing a significant boost
to performance.

We direct DPDK to use pages backed by NVM for packet buffers. DPDK uses a
concurrent memory allocator based on reference counting - used by both the NIC
driver and CPU cores. This means that packet allocation and deallocation does
not happen in the same order as their corresponding position in the replicated
RAFT log. To deal with this, we use a level of indirection as shown in
Figure~\ref{fig:nvm_log}. The NVM log is maintained as a circular log of fixed
sized pointers. Adding a level of indirection in the NVM log allows us to
separate the FIFO ordered circular log being manipulated by RAFT from packet
data being managed by the memory allocator of DPDK. Both the circular log and
packet data are in NVM. An advantage of this scheme is that it makes recovery
from NVM easy - appends to the circular pointer log are atomic and we can use
the pointer log to recover allocator state i.e. what pieces of NVM are currently
in use by the log.

\begin{figure}
\centering
\includegraphics[scale=0.3]{figures2/nvm_log.pdf}
\caption{NVM log structure}
\label{fig:nvm_log}
\end{figure}

\subsection{Eliminating Data Movement}
Software packet switches built on DPDK try to touch as little of the packet data
as possible, minimizing movement of data up and down the cache and memory
hierarchy. Log replication looks like multicast that does not require deep
packet inspection. Software packet switches implementing multicast therefore
simply manipulate packet headers to produce new packets to send on the output
ports. We designed our implementation of raft around the same principle.

\begin{figure}
\begin{verbatim}
    event_receive_client_req()
    {
      if(!check_is_leader()) {
        drop_msg
        return
      }
      Prepend raft log term and index
      Persist to local log
      Transmit to follower replicas
    }
\end{verbatim}
\caption{Cyclone event handling}
\label{fig:control_plane}
\end{figure}


The pseudocode in Figure~\ref{fig:control_plane} describes part of the packet
handling code in Cyclone organized as event handlers triggered on receiving a
packet at the leader. We focus on only one key event for brevity: the event
where a request is received from a client. The first step is to check that this
replica is indeed the RAFT leader in the current view (term). If not, the
message is simply dropped (a timeout causes the client to try a different
server). It then prepends RAFT related information to the packet - this includes
the current term and log index for this entry. Next, it appends a pointer to the
log in Figure~\ref{fig:nvm_log} effectively appending the packet to the
persistent log. Finally, it transmits the packet to follower replicas.

This entire process is done without making any copies of the received packet.
To illustrate how this is achieved, Figure~\ref{fig:packet_layout} shows how
Cyclone manipulates packet
layouts across the two steps of prepending a RAFT header and transmitting to
follower replicas. DPDK describes packets using an ``mbuf'' data
structure. Roughly speaking, an ``mbuf'' consists of a flat array of bytes
actually containing the packet and a fixed size piece of external metadata that
describes various aspects of the packet, most crucially a pointer to the start
and end of the packet in the byte array. DPDK's userspace drivers receive
packets from the NIC such that they are offset in the byte array by a
configurable amount referred to as ``headroom''. We strip off the existing
network headers in the packet and prepend RAFT related information specific to
each log entry in the headroom by shifting the start pointer appropriately.
These operations are standard enough for software packet switches that DPDK
provides convenient library calls for it. For the final step, we need to prepare
the packet for transmission to the various follower replicas. To do this we
prepare a different packet containing an ethernet header for \emph{each}
targeted replica and ``chain'' the data packet to each of these headers. Each
header is then separately handed off to the driver for transmission via the NIC,
carrying the data packet with it by association.

\begin{figure}
  \centering
  \includegraphics[scale=0.35]{figures2/network_packet.pdf}
  \caption{Cyclone network packet layout}
  \label{fig:packet_layout}
\end{figure}

Finally, we turn our attention to the persistence step in
Figure~\ref{fig:control_plane}. RAFT requires that the log entry be persisted
before it is multicast out to follower replicas. DPDK userspace NIC drivers
operate in DDIO mode~\cite{ddio} where the packet is directly written into the
CPU cache rather than first being DMAed into DRAM and then fetched by the CPU on
demand. Since the NVM is directly attached we need to persist the packet to it
by executing a cacheline flush ({\tt clflush}) instruction for every cacheline
in the packet and the pointer in the pointer buffer to persist these via the
memory bus. This is not too onerous a burden because we can use the newly
introduced {\tt clflush-opt}~\cite{clflush_opt} instruction specifically
intended to efficiently flush to persistent memory without the overhead of the
serialization normally introduced by {\tt clflush}. This allows us to hit full
memory bandwidth on present generation platforms, a quantity in excess of 200
Gb/s per core, well above the near term speeds of network interface cards. We
execute a single serializing {\tt sfence} before the sequence of {\tt clflushes}
to make sure any dirty cacheline due to header manipulation related writes from
the CPU are sent to cache.

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{figures2/batching.pdf}
  \caption{Batching}
  \label{fig:batching}
\end{figure}

Although RAFT is an efficient consensus protocol in the common case, the
protocol state machine still adds significant overhead to each packet, relative
to the time for the packet to flow in from the NIC and back out to the replicas.
We address this problem for the \emph{loaded} case using batching - treating a
whole sequence of client commands as a single RAFT log entry, while avoiding any
copies to group these packets together. Figure~\ref{fig:batching} illustrates
how this is done. We use a burst receive call available in the DPDK userspace
driver to receive a burst of client packets at a time. We then chain these
packets together and treat them as a single log entry from the perspective of
RAFT, amortizing the control plane overheads over the packets (at most 32 at a
time due to current driver limitations). Crucially, batching in Cyclone does not
involve a latency-throughput tradeoff like in many other
systems~\cite{ix-dataplane}. The batch receive call we use in DPDK returns
immediately with whatever number of packets is available, including zero. We
always flush the transmit buffer after every call to DPDK to transmit packets to
replicas. Therefore, we never tradeoff latency for throughput when batching. 


\section{Parallelism}
\label{sec:parallelism}

\begin{figure}
  \centering
  \includegraphics[scale=0.35]{figures2/race.pdf}
  \caption{Race with multiple physical logs}
  \label{fig:race}
\end{figure}



\section{Horizontal Scaling}
\label{sec:horizontal}
The basic principle in Cyclone is to turn log replication into a software packet
switching problem. A key ingredient in software packet switching is applying
multiple CPU cores to parallelize the control plane, scaling message processing
throughput. In this section, we describe how we can horizontally scale Cyclone's
single \emph{logical} log across multiple \emph{physical} logs. For each
physical log, we run an independent version of RAFT with the corresponding
control plane mapped to a dedicated CPU core. Further, we assume a concurrent
key value store where a set of CPU cores independently run operations from the
physical logs on the shared memory key-value store data structure, synchronizing
among themselves as necessary.

We begin by showing how we divide CPU cores and NIC resources on the plaform across
replication and key value store work, and show how a KV store request flows
across them.  We then describe how a key-value store API can be mapped to
independent physical logs without divergence in replica state. Finally, we
describe special handling in Cyclone for multi-key operations.

\subsection{Core Allocation}
A key-value store integrated with Cyclone runs two types of threads. The first
type of thread does key-value store work. This includes servicing lookup and
update requests and background threads that flush in-memory data structures to
secondary storage and do maintainence work on secondary storage data structures
such as compacting log-structured merge trees~\cite{lsmtree}. The second type of
thread is a dedicated thread per-physical log to run the corresponding control
plane including the associated RAFT state machine. We use static assignment of
threads to cores as recommended for packet switching libraries such as DPDK for
the best possible performance. Therefore we use the terms thread and core
interchangeably in this paper. Client requests are always received by the
control plane core for the physical log the request is directed to. A response
for the core is always sent by the KV store core that finally handles the
request.

An important resource for Cyclone is the NIC that must be shared among the
control plane cores that receive and replicate requests and the KV store cores
that send back responses. Most NICs provide independent transmit/receive queue
pairs and we aim for synchronization free access to these to aid horizontal
scalability with respect to the network resource. DPDK already provides fairly
scalable access to NIC queues, with the queue number being a parameter to send
and receive calls. All that remains for us is to map the queues to the control
plane and KV store cores, keeping in mind that there may be multiple NICs on the
system. We do this by iterating over all control plane cores followed by all KV
store cores. We assign two queue pairs to each control plane core, one to
receive requests from the client and the other to transmit and receive
replication protocol related messages. We assign one queue pair to each KV store
core to send back responses to clients. We are careful to spread the different
queue pairs for a core across the available NICs as far as possible. This is
important due to the asymmetry between the different types of traffic. Queue
pairs carrying replication traffic will likely be more busy than those carrying
client request responses due to the amplification in message count to multicast
the same message to different replicas.

The flow for an update operation in Cyclone is as follows. The network packet is
received by the control plane for the targeted physical log. The packet is then
immediately sent out for replication as described in the previous section. When
responses from a majority quorum is received, the request is passed to the
targeted KV store core for execution. The KV store core processes the request
and sends back the response to the client. We note that replication is pipelined
and decoupled from execution and thus multiple updates are usually in flight on
the network for replication at the same time.

The flow for a read operation is simpler. It is still targeted to a physical log
but instead of replicating it, the control plane core simply passes it on
straight away to the KV store core for execution and response.

The control plane cores communicate with the KV store cores through a one-way
lock-free FIFO queue passing pointers to packets. We leverage the convenient
ring library in DPDK for this.

Finally, an asynchronous source of work in the system is maintenance of the
two-level log. We depart from a 1:1 mapping between the first level NVM log and
the second level flashlog here. Each physical log consists of one NVM log
managed by the corresponding control plane core and multiple flashlogs each of
which is managed as an additional responsibility by a KV store core. We
therefore enforce a many to one mapping between key value store cores and a
front end control plane core - thereby partitioning KV store cores across
control plane cores. A request picks a control plane core and any one of the
associated KV store cores for execution. The request is logged in the NVM log of
the control plane core and garbage collected once it is drained to the flashlog
of the selected KV store core.

\subsection{Leveraging API Commutativity}
Update requests to a key-value store include a key and an associated operation -
such as ``put'' that sets the value for the key or ``merge'' that calls a
user-defined operator to merge a new value for the key into the existing value.
A naive approach that maps a key to a random physical log and subsequently to a
random core on the system for execution runs into the determinism problem -
replicas could diverge with respect to the final value assigned to a key on
different replicas. We illustrate this problem with an example in
Figure~\ref{fig:race} where two updates to the same key race with each other on
different physical logs and execution cores to end up being applied in different
orders on different replicas, leading to a divergence of state.

The solution to this problem is observe that the key-value store is a concurrent
data structure that ultimately admits a serialization of operations to
it. Further, consecutive operations to different keys in the serial history can
be interchanged, because operations to different keys are
\emph{commutative}. The correct way to use different physical logs is therefore
to simply hash the key to select the physical log \emph{and} the core on which
the operation executes. This guarantees determinism as the serial history of
two different replicas can be transformed into the same serial history by
reordering operations to different keys. Operations to the same key are already
in the same order in the serial histories by virtue of being replicated on the
same physical log and being executed on the same KV store core.

Further relaxations to the mapping scheme are possible when operations to the
same key are also commutative. A good example is when key value stores are used
to maintain counters. Different increment operations to the same counter are
commutative if one is prepared to tolerate non-determinism in the sequence of
updates seen by a reader with the counter reaching the same eventual value. This
specific use-case, in fact, lead to the addition of the merge operation in
Rocksdb, a key value store that we use for evaluation. Merge operations
therefore could be dispatched to a random physical log with no harm. For this
paper however, we stick to the stricter case that operations to the same
key are not commutative. However, Cyclone's client side libraries leave it to
the user to specify the exact physical log and key-value store core to use in
order to enable more relaxed mappings when necessary.

We note that commutativity as assumed by Cyclone is an \emph{API level
  property}. There is no assumption as to how application cores synchronize to
access the shared key-value state or the underlying data structures for holding
key-value pairs. We emphasize that this means replicas ultimately have the same
content but not necessarily the same memory state.

Finally, Cyclone provides seraializable reads but does not - at the moment -
guarantee the ability to read one's own writes, repeatable reads or more
generally, linearizable reads. The reason is that on a failover the new leader
replica might still have update operations in its NVM logs that are yet to be
applied to the key value store. However the original failed leader replica might
have applied these updates and a client might have already observed them. Users
who desire linearizable reads can issue a special API call to ensure that reads
are also routed through the logs. Flushing the NVM logs on failover is a better
solution that we are currently in the process of engineering into Cyclone.

\subsection{Ganged Operations}
Key value stores (such as Rocksdb) usually provide a batched write operation
that atomically updates a set of key-value pairs. A batched write is often a key
primitive for building more sophisticated operations such as
transactions. Batched writes require special handling in Cyclone due to the
requirement that we route a write request by hashing the key to select both the
control plane core (physical log) as well as KV store core. A batched write has
multiple keys and therefore we need to synchronize the progress of the operation
across multiple physical logs (and associated replication quorums) as well as KV
store cores. In effect, the serial history contains a grouped set of operation
on multiple keys as a single atomic operation. All serial histories across
replicas therefore continue to map to the same equivalent serial history modulo
reordering of operations to different keys.

One way to do this is to use a distributed transaction, using
two-phase commit to couple the otherwise independent replication quorums and KV
store cores. However this method has a number of problems. First, it introduces
an additional network round trip that is normally not necessary for a key value
store with a single shared memory image holding all the keys. Second, handling
failures with distributed transactions would have caused an explosion in
complexity in Cyclone, something we wished to avoid in the interests of
practical deployment to independent end users who need to understand and 
maintain Cyclone as part of their embedded KV store infrastructure.

Our solution instead is to use a technique we term ganged operations. It is
based on the observation that in the failure free case, executing a batched
write across multiple control plane cores and KV store cores does not look very
much different from a synchronous shared memory barrier. The challenge is to
effectively manage failures in replication without causing participating cores
to stop making progress. The first step is to reconfigure RAFTs leader election
protocol to ensure that the leaders for all the physical logs are co-located on
the same host. Clients always dispatch batched writes to a fixed control plane
core (called the injection core) which is then responsible for forwarding the
request to the participating control plane cores (for corresponding physical
logs). The injection core uses DPDK's packet cloning primitives to avoid
dataplane work while forwarding copies of the same batched write packet to
multiple control plane cores. The injection core also adds a ``nonce'' - a
unique timestamp to the packet. In addition, the injection core adds a unique
view number to the packet containing the term numbers of all the participating
RAFT instances (read from shared memory of the co-located leaders).  The flow
for ganged operations is described in the pseudocode of
Figure~\ref{fig:ganged_ops}. We defer the discussion of how we generate the
nonce to later in this section. We also assume that a unique barrier is
allocated in shared memory for each ganged operation. We discuss later how this
is done without using dynamic allocation.

In the absence of failures Figure~\ref{fig:ganged_ops} is straightforward. Once
replication is complete on all participating physical logs, the corresponding KV
store cores execute \\{\tt event\_replicated\_ganged\_op}. They use a bitmask
for the barrier setting the appropriate bit for themselves. Once all KV store
cores have arrived at the barrier the operation is executed. A distinguished
leader core (we use the minumum numbered participating KV store core) is
responsible for sending back the response to the client. Two types of failure
need to be considered. First, replication on the physical log corresponding to
one of the non-leader cores could fail. The remaning cores would never progress
past the barrier. To solve this the leader core monitors the remaning cores for
their current view - which is the RAFT term of the last executed log entry. If
the view has moved past the view in the packet, replication failed on that
physical log. The leader then sets the bit for the core that will no longer
participate in the barrier to aid progress (a technique we borrowed from
lock-free data structures~\cite{lock-free}). It also sets a failed flag to
signal failure to the other participating cores. The second failure case is that
the leader core might not receive the operation if it fails during replication
from corresponding physical log. This case is detected in a similar manner by
the remaining cores by monitoring the published view from the leader core in
shared memory. We add that on a leadership change due to timeout we enqueue and
execute a nop on the physical log and all associated KV store cores to ensure
previous entries are committed - a corner case in the RAFT protocol.


\begin{figure*}
  \centering
\begin{verbatim}
    // Control plane core
    event_recv_ganged_op(packet)
    {
      Begin replication of packet
    }

    // KV store core
    event_replicated_ganged_op (packet, barrier)
    {
      if(leader KV core)
        atomic set bit me in barrier.mask         
        do
         for each KV store core in packet
          if core_public_data[core].view > packet.view
           barrier.failed = true 
           atomic set bit for core in barrier.mask
        while barrrer.mask != mask of participating KV cores
        if barrier.failed
         Ganged replication failed. Send retry to client. return.
        else       
         execute operation
         send response
      else
        wait until 
         core_public_data[leader core].view > packet.view OR
         barrier.mask == mask of participating KV cores
        if core_public_data[leader core].view > packet.view OR
           barrier.failed
         Ganged replication failed. Send retry to client. return.
        else
         execute operation
    }
\end{verbatim}
\caption{Ganged Operation}
\label{fig:ganged_ops}
\end{figure*}

The assumption in Figure~\ref{fig:ganged_ops} is that each ganged operation is
mapped to a unique barrier. We achieve this by using a fixed piece of memory
owned by the leader core to hold the barrier and write the nonce to it in order
to indicate that the barrer is active for the corresponding batched write
operation. Non-leader cores watch for the nonce to know when to execute the
ganged operation barrier in Figure~\ref{fig:ganged_ops}, while also monitoring
the leader's published view to detect the case where the ganged operation fails
to replicate on the leader's physical log.

Finally, we describe how we generate the nonce. The nonce is generated on the
injector core by concatenating the ethernet MAC ID of the first NIC on the
system with a 64 bit value that is the number of CPU timestamp counter cycles
since epoch time (read from the real time clock at startup plus the number of
cycles from the CPU {\tt rdtsc} instruction). The nonce can only be repeated if
the same machine manages to fail and come back up in less time than the real
time clock drift (controlled with NTP), a possibility that we discount.

Ganged operations possibly consititute the most complex part of Cyclone but the
code weighs in at well under a couple of hundred lines. We believe that
this additional complexity is still small compared to distributed transactions.

\section{Evaluation}
\label{sec:evaluation}
We evaluate Cyclone on a 12 node x86 Xeon cluster connected via a 10 GigE
switch. Three of the machines are equipped with 1.6TB Intel DC P3600 SSDs and
4*10 GigE ports. The remaining nine machines do not have SSDs and have only one
10 GigE port, serving as clients for most of the experiments. We turn on jumbo
frame support in the 10 GigE switch to enable maximum use of batching in
Cyclone. As with other work~\cite{faast}, we use DRAM on the machines to proxy
for NVDIMMs where necessary - the persistent memory needed never exceeds 64 MB
regardless of the size of the key value store or second level log on flash. We
divide the evaluation into three parts. First, we evaluate Cyclone's performance
with a single level log as a pure software packet switch. Next, we evaluate
performance when adding a second level of log on flash. Finally, we evaluate
performance when integrated with Rocksdb~\cite{rocksdb} as an alternative to
Rocksdb's write ahead log. Unless otherwise mentioned, we use a 60 byte header
followed by an optional payload for experiments. We log both the header and
payload. In all cases the server echoes the received entry (header and payload)
back to the client. We fix the number of KV Store threads at 32 using at most 8
physcial logs with associated control plane cores for running RAFT instances.

Cyclone is built on top of DPDK to apply software packet switching techniques to
the log replication problem. We therefore begin by systematically evaluating
optimizations applied in Cyclone to replicate the NVM log in
Figure~\ref{fig:network_opts} - with no payload. The y-axis reports latency seen
at the client (which means two network round trips with replication). Using
TCP/IP to replicate a RAFT log tops out at around 30K entries/s.  Switching to
DPDK (the line marked +DPDK) improves the throughput by an order of magnitude to
around 500K entries/s. Using batching (the line marked +batching) improves the
performance further bringing us close to a million entries/s. Scaling
horizontally to 8 physical logs (+8 phy logs) improves performance to close to
2M entries/s. Finally using all 4 ethernet ports on the machine to replicate
entries improves performance considerably to 6M entries/s. In all, performance
improves by 200X over the TCP/IP single log baseline. Cyclone also considerably
improves the latency for replication, from close to 100us with TCP/IP to around
30us at peak throughput.

\begin{figure}
\includegraphics[scale=0.6]{results2/network_opts.pdf}
\caption{Network optimizations for top level log}
\label{fig:network_opts}
\end{figure}

There are two factors that can have significant impact on Cyclone's
performance. First, the number of replicas dictates the outgoing message rate
from the leader replica and therefore increasing the replication factor can
decrease Cyclone's performance. Figure~\ref{fig:replicas} shows the impact of
varying replica count. Using only a single replica cuts out a network round trip
and shows the best unloaded latency (10 us) and peak throughput (near 10M
entries/s). Adding replicas decreases the peak throughput down to around 2M
entries/s with 5 replicas. We note that a number of previous pieces of
work~\cite{faast, farm} use three replicas and therefore we focus on three
replicas for the replicated cases we consider below. The second factor that
dictates Cyclone's performance is the size of the log entry being
replicated. Figure~\ref{fig:payload} shows the effect of increasing the payload
size from zero to 512 bytes. Peak throughput drops from 6M entries/s to
approximately 2M entries/s. At this replication rate, the leader replica needs
to transmit data at approximately 30 Gbit/s. Coupled with the cost of network
headers all four 10 GigE ports are now saturated and therefore Cyclone hits the
network line rate bottleneck at this point.

\begin{figure}
\includegraphics[scale=0.6]{results2/replicas.pdf}
\caption{Impact of replica count}
\label{fig:replicas}
\end{figure}

\begin{figure}
\includegraphics[scale=0.6]{results2/512.pdf}
\caption{Impact of payload size}
\label{fig:payload}
\end{figure}

We now turn our attention from the network component of Cyclone to the storage
one by adding the second level flashlog. We evaluate the impact of adding
block storage to our hitherto pure packet switching scenario in
Figure~\ref{fig:flashlog}. Batching entries from the top level NVDIMM log to the
second level flashlog is clearly beneficial as adding flash storage at the
second level has almost no impact on peak performance in Cyclone for small
entries. The situation however changes for larger
entries. Figure~\ref{fig:flashlog_512} shows that using a 512 byte payload has a
significant impact on peak throughput - it drops to approximately 350K
ops/sec. This corresponds to around 50K 4KB IOPS to the SSD to write out the
flashlog pages. The peak for the drive is 160K IOPS using a queue depth
(concurrency) of 128. With 32 application threads we expect a lower peak
throughput around 40K IOPS explaining our bottleneck at 50K IOPS.  It is
possible to tune our observed performance further by aligning the flush boundary
to increase the number of outstanding requests - we do not do so in this paper,
keeping Cyclone agnostic to the exact size of entry being replicated. A final
point about Figure~\ref{fig:flashlog_512} is that once we are past the storage
bottleneck the latency spike is dramatic and large enough to trigger Cyclone's
failure detector and repeated retries from the clients. There are - therefore -
no points on the ``knee'' of the curve as in the pure packet switched one-level
log case.

\begin{figure}
\includegraphics[scale=0.6]{results2/flashlog.pdf}
\caption{Impact of adding second level log}
\label{fig:flashlog}
\end{figure}

\begin{figure}
\includegraphics[scale=0.6]{results2/flashlog_512.pdf}
\caption{Second level log with 512 bytes payload}
\label{fig:flashlog_512}
\end{figure}

The final dimension we evaluate is of ganged operations. The primary purpose of
ganged operations is to avoid the need for distributed transactions to
manipulate what is a single shared memory image and therefore we were most
concerned about unloaded latency given the complexity of synchronizing different
replication quorums as well executing our rendezvous protocol on multiple
cores. We therefore setup an experiment where a single client - reflecting the
unloaded case - made ganged requests to the replicas. We varied the number of
cores participating in the request from one to the full complement of 32
application cores. Figure~\ref{fig:ganged} shows the results both using a single
level log as well as a two level log. The primary takeaway is that unloaded
latency increases slowly as we increase the number of active cores - to around
40 us from the baseline of 20 us. There are two causes of this. First, there is
a quicker rate of increase from 1 to 8 active cores, the reason being that this
corresponds to an increase in the number of physical logs that must be
synchronized on. There is some amount of synchronization involved in the
userspace DPDK driver for access to common resources for accessing the NIC and
this affects the ability to simultaneously issue replication messages on all
quorums. The smaller contribution to increasing latency that continues past the
8 core case is due to cacheline pingponging when executing the rendevous of
Figure~\ref{fig:ganged_ops}. Both these sources of latency drift could be
corrected using replication quorums mapped to dedicated NICs and more scalable
rendezvous designs (such as with machine aware broadcast
trees~\cite{broadcast_tree}). However we deemed the complexity of such
optimizations unnecessary. The added latency for most cases is well under the
extra round trip delay (including replication) for all phases of a 2PC.

\begin{figure}
\includegraphics[scale=0.6]{results2/multi.pdf}
\caption{Ganged Operations}
\label{fig:ganged}
\end{figure}

We now evaluate Cyclone integrated with the Rocksdb persistent key value
store. Rocksdb is a complex industrial strength persistent key value store and
this means that it is accompanied by a complex array of performance tuning knobs
to get the best performance from flash. We were also aware that SSD performance
is slated for dramatic increases in the coming years with the introduction of
new memory technology such as 3DXPoint based Optane SSDs. To demonstrate that
Cyclone is future proof we eliminated flash media performance from the picture
as far as the key value store is concerned by placing all files for the key
value store (SSTables) on a RAMdisk, which presumably represents the limit in
performance for flash in the near future. All log files on secondary storage
however - both Rocksdb's own write ahead log and the alternative of Cyclone's
second level flashlog - are placed on the SSDs.

We evaluate two different request sizes: 8 byte keys with 8 byte values and 8
byte keys with 256 byte values. We run 8 physical logs on 8 cores and devote 32
cores to Rocksdb. Since we are interested in performance of the log, which is
only used for update requests, our workload is 100\% writes.

Before evaluating with Rocksdb, we measure the baseline performance of
replicating the log with Cyclone for the given request sizes - Rocksdb performs
a no-op. We note that in addition to the key and value, we are also logging
RocksDB specific request data such as operation type and the request header.
Figure~\ref{fig:kv_baseline} shows the baseline performance for the chosen
request sizes. With the smaller request size, Cyclone can conservatively sustain
close to a million requests a second at a latency of just under 25us. With the
larger request size, Cyclone can sustain around 350K requests a second, again at
a latency of just under 25 us. Armed with these baseline numbers we now examine
how well Cyclone performs with Rocksdb.

\begin{figure}
\includegraphics[scale=0.6]{results2/kv_baseline.pdf}
\caption{Baseline KV replication performance}
\label{fig:kv_baseline}
\end{figure}

The performance of Rocksdb with Cyclone for the small update workload is shown
in Figure~\ref{fig:rocksdb} - essentially presenting the solution to the problem
description graph in Figure~\ref{fig:problem}.  We consider four different
setups. The line labeled 'rocksdb' is the key value store running with no
logging whatsover - a system crash would lead to data loss. The line labeled
'rocksdb/wal' is for Rocksdb running with its write ahead logging turned on. The
large gap between these two is the overhead of the existing Rocksdb WAL
solution. The line labeled 'rocksdb/Cyclone 1 way' is a two level Cyclone log
but without any replication. The line almost exactly tracks the performance of
Rockdb. As suggested by the baseline replication performance, Cyclone is able to
provide a write ahead log with no overhead to Rocksdb. The line labeled
'rocksdb/Cyclone 3 way' is with 3-way replication turned on. Other than a 20us
delta due to the extra network round trip, the line almost exactly tracks
Rocksdb performance with no logging. Cyclone therefore provides high
availability to Rocksdb at a fraction of the cost of its existing single machine
write ahead log. We also repeat the experiment for the larger update size in
Figure~\ref{fig:rocksdb_256}. The conclusions are identical: Cyclone solves
Rocksdb's write ahead logging problem.

\begin{figure}
\includegraphics[scale=0.6]{results2/rocksdb.pdf}
\caption{Rocksdb - small updates}
\label{fig:rocksdb}
\end{figure}

\begin{figure}
\includegraphics[scale=0.6]{results2/rocksdb_256.pdf}
\caption{Rocksdb - large updates}
\label{fig:rocksdb_256}
\end{figure}

Next, we consider the problem of supporting Rocksdb's write-batch operation that
atomically writes a set of key-value pairs into the KV store. In order to
perform the operation with Cyclone managing the log, the client must issue a
ganged operation across cores owning the keys in the write batch. This is in
contrast to baseline Rocksdb where the entire write batch can be sent to any
core. A key concern here was whether Cyclone would add any latency to the
operation due to the extra synchronization needed across replication quorums and
participating application cores. We examine the problem for the unloaded case
and small updates in Figure~\ref{fig:rocksdb_multi} for a single client with
increasing number of keys in the batch - till 32 keys that covers all
application cores. The line labeled Rocksdb is with no write ahead logging. We
note an increasing latency for this baseline indicating Rocksdb takes longer
with larger key batches. The existing option of Rocksdb/wal has considerably
larger latency. Cyclone does an effective job of cutting down on this latency
even as it needs to pay a price for synchronizing multiple quorums and
application cores making it somewhat slower than running Rocksdb with no logging
for batched writes. A distributed 2PC transaction still remains more expensive
just in terms of network delay than using Cyclone.

\begin{figure}
\includegraphics[scale=0.6]{results2/rocksdb_multi.pdf}
\caption{Rocksdb - batched writes}
\label{fig:rocksdb_multi}
\end{figure}

Finally, we showcase the benefit of using Cyclone beyond pure performance as
compared to the existing single machine Rocksdb write ahead log. Cyclone brings
multi-machine availability with the ability to automatically failover. We
demonstrate this in Figure ~\ref{fig:timeline} that shows the timeline of a run
where we kill the server process on the leader replica. Cyclone is configured to
with a 30ms failure detection timeout after which the client library tries
replicas in turn to locate the leader - in this case it fails over in about
60ms.

\begin{figure}
\includegraphics[scale=0.6]{results2/failover.pdf}
\caption{Rocksdb - failover}
\label{fig:timeline}
\end{figure}

\section{Related Work}

Cyclone takes a software only approach to improving replication performance
using commodity network hardware. At the other end of the spectrum, Consensus in
a box~\cite{consensus_box} implements the Zookeeper atomic broadcast protocol
together with a NIC on an FPGA. That work ignored durability focusing purely on
replication performance. Latency numbers are impressive due to the FPGA that cuts
out the path from CPU caches to the NIC and are reported to be as low as 3us for
a round trip from leader to follower replica compared to the 7us we observe with
DPDK. Notably however they reported 7us when using TCP/IP from the FPGA rather
than their own connection oriented network protocol. On the other hand peak
replication throughput appears to be 6M replications/sec on 66 Gbps of aggregate
network bandwith, comparable to the mark obtained by Cyclone on an aggregate 40
Gbps of network bandwidth. We believe the general applicability afforded by our
software only packet switching solution makes it a compelling alternative to an
FPGA based solution even given the larger latency.

Network Ordered Paxos~\cite{nopaxos} reprograms top of rack switches to order
client requests and multicast them to replicas - attacking replication latency
by removing the need to direct requests through a leader replica. It is hard to
compare performance due to the 1Gbps links used to leaf nodes in NoPaxos but we
note that avoiding changes to switching infrastructure was a specific constraint
in the design of Cyclone due to the difficulty we encountered in convincing
public cloud customers to allow such changes to be made.

{CRANE}~\cite{crane} is a system for replicating mutithreaded programs using
Paxos. It uses deterministic multi-threading to ensure replicas converge to the
same state as opposed to semantic equivalence in Cyclone using the commutativity
of the key value store interface. Although CRANE is more generally applicable in
that it makes no assumptions about the application API, deterministic
multithreading comes with high overheads for applications that store large
amounts of data - CRANE reports around a 2X slowdown for MySQL likely rendering
it unsuitable as a replacement for the write ahead log in key value stores.

A number of researchers have proposed using high performance networking hardware
for improving the performance of various distributed system primitives such as
distributed transactions~\cite{farm} and replication~\cite{dare, mojim}. We note
that the ideas in Cyclone are not specific to ethernet. The same ideas of packet
switching and horizontal scaling can be equally applied to RDMA. However, we
chose commodity ethernet due to short term deployment plans that are a
priority design target. 

The scalable commutativity rule~\cite{scalable_commutativity} generalizes the
idea of commutativity in key value store interfaces that we have used for
horizontal scaling in Cyclone. Conveniently, it provides a tool for checking when
API calls can commute in a history. It is a simple exercise to express a key
value store API in COMMUTER and check that operations to different keys
commute. This provides a theoretical basis for horizontal scaling, something we
have omitted for brevity from this paper. The scalable commutativity rule also
suggests that scalable key value store implementations would necessarily have
commutative interfaces, something we confirm and exploit for key
value stores.

There is an in progress transformation of durable memory in datacenters
with the advents of 3DXPoint. Researchers have speculated on the impact of cheap
and high volume directly attached 3DXPoint~\cite{tiering}. Memory vendors have
also begun shipping software libraries such as NVML~\cite{nvml} for building durable
in-memory data structures using undo logging. A durable key value store using
3DXPoint would be an idea target for Cyclone because each NVM log entry can be
discarded as soon as it is applied to the persistent key value store removing
the need for the flashlog. This would allow Cyclone to operate at pure packet
switching speeds, a goal we hope to achieve in the future with a single level
log for NVML data store applications.

Cyclone provides a client library that automatically times out and retries
requests to deal with failure. Protection against repeated execution of the same
request is currently left to the key value store and client, since we felt that
exactly once semantics is optional when updates such as put operations are
idempotent. We have experimented with exactly once semantics as a basis for
distributed transactions across shards running Cyclone integrated key value
stores, along the lines of similar work~\cite{raft_lin}, using a fixed client
space and maintaining information about last seen sequence numbers from clients
in NVM. Distributed transactions on Cyclone shards however are outside the scope
of this paper.

Finally, we note that scaling a log horizontally for concurrency is also an idea
adopted by filesystems. NOVA~\cite{nova} is an example that uses a per-inode log
to improve concurrency when the filesystem is placed on NVM attached to the
memory bus. Cyclone also uses a distinct NVM log per control plane core in a
similar vein but does not maintain a namespace since the number of NVM logs is a
small fixed number equal to the number of control plane cores.

\section{Conclusion}
Persistent key value stores struggle today to provide high availability using a
write ahead log without a significant impact to performance. Cyclone solves this
problem by leveraging a small amount of directly attached non-volatile memory
that transforms the replication problem into a software packet switching
problem. Cyclone and its Rocksdb bindings are slated for release to the open
source community and we hope both the code and the ideas in the system itself
will help key value store builders and researchers overcome their logging
problem.
\newcommand\myurl[2]{\url{#1}}
\bibliographystyle{plain}
\bibliography{paper}

\end{document}




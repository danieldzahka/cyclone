\documentclass[twocolumn]{article}
\usepackage{times}
\usepackage{parskip}
\begin{document}
\title{Cyclone: Fault Tolerance Middleware for NVM Clusters}
\author{Amitabha Roy et al. \\ amitabha.roy@intel.com}
\maketitle
\begin{abstract}
Memory technology in the datacenter is due to undergo a paradigm shift with the
introduction of directly addressable non-volatile memory both in the form of
battery backed non-volatile DIMMs as well as newer memory types such as
3D XPoint. Datacenter applications typically use storage devices to survive
faults that take one more machines offline by combining the persistence offered by
durable storage with availability through replication. Building such fault
tolerant applications is hard and users are forced to either build
them from scratch or modify their applications to fit the interfaces of solutions
such as zookeeper. In addition to a loss of flexibility such applications today
are unable to take advantage of the speed of directly attached non-volatile
memory as opposed to older slower storage in the form of disks and SSDs.

This paper presents Cyclone - fault tolerance middleware that exposes a
replicated and durable heap. This heap can be directly manipulated using loads and
stores. Cyclone provides both failure atomicity as well as
strongly consistent replication when updates to the heap are enclosed in
software transactions thereby giving strong guarantees to programmers
with minimal effort on their part. Under the hood Cyclone decouples modification
of local persistent memory from replication to maximize performance. Unlike
traditional systems that provide strong replication, the ``in-memory'' image is
persistent and there is no need for checkpoints - except when new replicas are
added. In addition linearizable reads can be directly executed without the need to
combine pending updates from a log.
\end{abstract}  

%\section{Introduction}

\section{Single Node Programming Model}
Cyclone provides users with a replicated and durable heap that can be
manipulated via a set of library routines. Cyclone builds on top of Intel's Non
Volatile Memory Library (NVML) and provides transparent replication to programs
written to use NVML for persistence on a single node. This section therefore
describes NVML's programming model.

NVML provides special functions to allocate and free persistent memory as well
as fat pointers to persistent memory to ensure that referential integrity in the
heap is maintained across restarts - since the virtual memory ranges into which
persistent memory is mapped may change.

\begin{figure}
{\scriptsize
\begin{verbatim}

struct ll_node {
  int value;
  TOID(ll_node) next;
};

void insert_after(TOID(ll_node) prev, 
                  TOID(ll_node) new_node)
{
  D_RW(new_node)->next = D_RO(prev)->next;
  D_RW(prev)->next = new_node;
}

\end{verbatim}
}
\caption{A Persistent Linked List of Integers}
\label{fig:example}
\end{figure}

Figure~\ref{fig:example} shows the example of a linked list node and a function
for inserting a new node after a given one. The {\tt TOID} declaration wraps the
linked list node type to produce a fat pointer to it. A {\tt TOID(ll\_node)}
type therefore is a pointer to a linked list node stored in persistent memory.
The {\tt insert\_after} function uses the {\tt D\_RO} and {\tt D\_RW} calls to
convert fat pointers into directly accessible read-only and read-write pointers
respectively. Other than the mechanics of using fat pointers the code entirely
resembles that which a programmer might write for a standard linked list in
DRAM.

\section{Failure Atomicity}
Programmers typically find it hard to reason about failures when writing
code. This is a problem when directly manipulating data structures in persistent
memory. Depending on the specifics of the system architecture, updates to
persistent memory can be persisted out of order thereby leaving the data
structure in an inconsistent state. To illustrate this consider a failure during
execution of the code to add a node to a linked list in
Figure~\ref{fig:example}. If either of the two updates to the next fields of the
previous and new node fail to be persisted, the linked list is left in a
disconnected state with one or more connected nodes inaccessible.

NVML provides tools for programmers to write code that is failure
atomic by allowing a block of code to be delimited as a transaction. A
transaction is either completely executed in terms of its effects on
persistent memory or not executed at all. Under the hood, NVML tracks all
updates to persistent memory in a persistent undo log - the log entries are
persisted before the update to the actual memory is allowed. On a failure, the
undo log is used to roll back incomplete transactions. A transaction is
considered complete when it completes execution and the updated objects are
flushed (automatically by NVML) to persistent memory. The log entries for the
transaction are then immediately deleted. The only burden placed on a programmer
for failure atomicity is to delimit transactions and that they must inform NVML
about objects that are updated. Figure~\ref{fig:example_fatomic} shows how to
convert the function for adding a node to a linked list into a failure atomic
one. The only additions are the transaction delimiters and the {\tt TX\_ADD}
calls to mark the objects being updated \emph{before} the actual updates to
them. Figure~\ref{fig:undo_log} shows how Cyclone maintains the undo-log of
objects.

\begin{figure}
{ \scriptsize
\begin{verbatim}

struct ll_node {
  int value;
  TOID(ll_node) next;
};

void insert_after(TOID(ll_node) prev, TOID(ll_node) new_node)
{
  TX_BEGIN {
    TX_ADD(new_node);
    D_RW(new_node)->next = D_RO(prev)->next;
    TX_ADD(prev);
    D_RW(prev)->next = new_node;
  } TX_END
}

\end{verbatim}
}
\caption{Failure Atomic Peristent Linked List}
\label{fig:example_fatomic}
\end{figure}

\begin{figure}
{ \scriptsize
\begin{verbatim}
TX_BEGIN()
{
  Initialize undo log
}

TX_ADD(o)
{
  if(o not already in undo log) {
    Append <o, value of o> to undo log
  }
}

TX_END()
{
  for each entry <o, v> in undo log {
    flush o to persistent storage
  }
  truncate undo log
}

Recover()
{
  for each entry <o, v> in undo log {
    set o to v
    flush o to persistent storage
  }   
}

\end{verbatim}
}
\caption{Undo Log in Cyclone}
\label{fig:undo_log}
\end{figure}

A crucial feature of NVML is that it abstracts the details of the persistence
mechanism away from programmers. This allows the same code to be deployed on
clusters with a range of persistence mechanisms - mapping pages from secondary
storage such as SSDs, using battery backed NVDIMMs, using an onboard battery to
save DRAM state to SSD on failure [FARM] or using directly accessed persistent
memory [PMFS]. Each of these options has different requirements. For example the
NVM used in PMFS requires explicit cache flush (CLFLUSH) and PCOMMIT
instructions. NVDIMMs would conceivably require only a CLFLUSH to flush the
cacheline and ensure that the line enters the asynchronous data refresh
domain. Using secondary storage is also an option with faster 3DXPoint storage
behind an NVMe interface - this would require an {\tt msync} and OS pagecache
intervention to persist data to secondary storage. NVML seamlessly uses the
right persistence mechanism without requiring code using the library to be aware
of these details.

\section{Replication}
Cyclone replicates the state of the NVML heap across multiple machines in a
cluster. Cyclone provides strong consistency - a majority of machines in the
quorum of replicas always agrees on the current state of the NVM heap. Unlike
traditional approaches to strong replication however, Cyclone is designed to
decouple replication from execution of user code modifying NVM. It does so by
encapsulating changes to the NVM heap in server side code driven by client RPC
requests. We chose this model since it closely reflects the use cases most
suitable for NVM-based clusters.

Cyclone replicates state \emph{by executing the same sequence of RPC calls on
  all members of the quorum}. By executing the same sequence of RPC calls on all
machines we end up with the same state\footnote{We discuss how to handle
  non-determinism in Section~\ref{sec:synch}}. Cyclone uses RAFT [cite usenix
  raft paper] under the hood for consensus on the log of RPC calls - although
any other similar state machine replication protocol such as
Paxos is also suitable. Cyclone provides the means to clients to achieve exactly
once semantics for the RPC calls they make by numbering RPC calls and
remembering the number and result of the last executed RPC call at the server end.

\subsection{RPC Call Numbering}
Cyclone is preconfigured for a fixed (large) number of clients. All RPC calls
from a client are numbered and Cyclone remembers (in a persistent manner) the
number and result of the last RPC call made by a client. A particular client can
exist on any machine - cyclone treats all instances of the client as a single
client. Cyclone rejects RPC calls that are not in sequence. Any RPC call is
executed exactly once across the quorum - regardless of failure and
reconfigurations. This provides the means for clients to easily achieve exactly
once semantics, provided the client does not crash. Cyclone can supply the last
executed RPC call number and its result to assist recovering and stateless
clients.

\subsection{Asynchronous Replication}
By default, Cyclone decouples execution of the RPC call on any replica from its
replication. This is particularly beneficial to performance since it allows
overlapping the update of non-volatile state with the network I/O associated
with replication. Making the call to the application and executing the it in a
failure atomic transaction takes a
significant amount of time and therefore it makes sense to overlap this with the
round-trip delay on the network for replication. This is possible in Cyclone
since we use failure atomic transactions that can be rolled back. Replication
can fail due to various reasons such as failure of nodes, a network partition or
a new leader getting elected in the underlying replication protocol
(we refer to this as a view change). Since failure atomic transactions can be
rolled back, we are able to speculatively apply the changes to NVM without
waiting for replication to finish.

The runtime steps for asynchronous replication are shown in
Figure~\ref{fig:async_rep}.

\begin{figure}
{ \scriptsize
\begin{verbatim}
Initiate replication of RPC call and arguments
TX_BEGIN {
  Execute RPC call and modify local NVM
  Block till replication result known
  if(replication failed) {
    TX_ABORT;
  }
} TX_END
\end{verbatim}
}
\caption{Asynchronous Replication}
\label{fig:async_rep}
\end{figure}

\subsection{Synchronous Replication}
\label{sec:synch}
Asynchronous replication provides good performance but lacks guarantees
occasionally necessary for distributed systems paradigms such as output commit
and the ability to integrate code that has non-deterministic execution paths
(such as being dependent on the current time) or side effects - such as making
further requests. Cyclone therefore provides programmers the ability to closely
synchronize execution across the replicas using synchronous
replication. Synchronous replication extends the programming model provided by
cyclone - but neccessitates some awareness on the part of the user about the
difference between the leader and follower replicas. We show later
(Sec~\ref{sec:examples}) how this awareness enables the crafting of efficient
fault tolerant applications with little effort on the part of the programmer.

Figure~\ref{fig:sync_rep} shows how synchronous replication operates.  Execution
of the RPC call \emph{waits until the call and its arguments have been
  replicated.} This crucial difference from asynchronous RPC execution ensures
that any externally visible side effects of the RPC such as calls to other
machines only happen after it is guaranteed that the effects of the RPC call
will be committed to the replicas. This provides the output commit property
necessary in some applications.

Next, the RPC call is executed on the leader and the result of the RPC call is
replicated to the followers. The followers then proceed to execute the RPC call
locally taking into account the result from the leader. This result can include
data to synchronize non-deterministic steps such as reading the time on the
replica.

This process is made conditional on the view (current configuration of
replicas and choice of leader) staying unchanged. To successfully complete
execution, the leader must be able to execute the RPC call and replicate the
result within the current view. This ensures that in the event of a network
partition and later repair, an old execution of a previous leader cannot commit a
log record with results from a synchronously executed RPC call.

\begin{figure*}
\centering
\begin{minipage}{0.7\textwidth}  
{ \scriptsize
\begin{verbatim}
Initiate replication of RPC call and arguments
Block till replication of RPC call returns
if(replication success) {  
  while (true) { // Must complete 
    view = current RAFT view (term)
    TX_BEGIN {
      if(is_leader && !have_execution_result) {
        Execute RPC call and modify local NVM
        replicate result in view;
        Block till result replication returns
        if(result replication failed) {
          TX_ABORT;
        }
      }
      else {
        Block till committed result replication or view changes
        if(!committed result replication) {
          TX_ABORT;
        }
        Execute RPC call to modify local NVM with result from leader
      }
    } TX_ONABORT {
      continue; // Try again
    } TX_END
    break; // All done
  }
}
\end{verbatim}
}
\end{minipage}
\caption{Synchronous Replication}
\label{fig:sync_rep}
\end{figure*}

\section{Concurrency}
\subsection{Network}
The single leader in RAFT is responsible for sending every log entry to every
replica. Sending log entries one by one or even in batches causes inefficient
use of the network link between the leader replica and followers when the leader
must wait for an acknowledgment from the follower before sending the next batch
of log entries. The solution is to send log entries from the leader as they are
appended to its log without waiting for an acknowledgment. The result however is
that entries can arrive out of order at follower replicas due to an unreliable
network.

We note that RAFT as specified already tolerates out of order arrival of log
entries at follower replicas (a newly elected leader assumed replicas are up to
date when it starts sending out log entries). However follower replicas can only
apply log entries \emph{in order}- dropping log entries that cannot be applied
to extend the log at a follower replica. To bridge this gap, with no changes to
the protocol, we added an in-memory cache of log entries (that we dub the log
cache) at follower replicas to cache log fragments that arrive out of
order. These are then applied opportunistically as soon as all preceding entries
have arrived.

Pseudocode for the log cache is shown in Figure~\ref{fig:logcache}.

\begin{figure}
{ \scriptsize
\begin{verbatim}
on ReceiveEntry(leaderTerm, entry) {
  if (leaderTerm < logcache_term) {
    drop entry // Stale
  }
  else if(leaderTerm > logcache_term){
    drop all entries from logcache
    logcache_term = leaderTerm
  }
  while(entry = canAdd(raftLog, logcache)) {
    AddEntry(raftLog, entry);
    RemoveEntry(logcache, entry)
    Send RAFT response for entry;
  }
}
\end{verbatim}
}
\caption{Logcache}
\label{fig:logcache}
\end{figure}

The log cache is built around the key property of RAFT that there is exactly one
leader in any term and the log at the leader of a term can only be appended to. We
therefore need only support adding entries to the log cache and flush the entire
log cache on detecting a change in leadership due to a term larger than that of
the leader whose entries are cached. Application of log entries follows the
normal RAFT protocol where the last entry in the log must match the desired
previous log index and previous log entry term specified in the new entry to be
appended.

The log cache enables leaders to send batches of log entries as soon as they are
available rather than waiting for an acknowledgment from followers for the
previous batch - enabling better utilization of the network link. We note
however that the leader continues to track the last log entry \emph{known} to be
replicated at any follower (the {\tt match index} in the RAFT protocol
specification) as usual and resends log fragments on timeouts - tolerating
unreliable networks that drop log fragments.

In addition to better utilization of the network link out of the leader replica,
the log cache also enables the leader replica to effectively offload the work of
replication. To do so it simply informs the client originating the log entry of
the list of replicas to which the entry must be sent. The client sends the
entries, collects the responses and responds to the leader replica as soon as a
majority quorum has responded. From the perspective of the RAFT protocol the
client simply acts as an unreliable router. The log cache allows follower
replicas to utilize chunks that might arrive out of order from different
assisting clients. The benefit of such a scheme is that the replication load on
the leader (in terms of network bandwidth) is distributed across the clients
with a larger load (more clients) permitting more offload. As we show in the
evaluation this is an effective technique to keeping tail latency under control
in Cyclone deployments that can tolerate the loss in performance due to the
extra hop but are willing to pay that price for better tail latencies.

\section{Example Application}
\label{sec:examples}
We now demonstrate how Cyclone can be used in practise by putting together an
example of a linearizable set of counters. The problem is to construct a sparse
set of counters indexed by a 64 bit key and support \emph{linearizable}
increment, decrement and an atomic set increment. The atomic set increment takes
as input a set of counter indexes and their current value and atomically
increments the counters from the the given values. 

Figure~\ref{fig:counter_set} shows a linearizable implementation executing on
servers. Odd counter values are considered unstable and even values stable.
Clients make RPC calls to servers the trigger the execution of the
appropriate function. Any client can make calls for the get and increment
operations shown on the left hand side. A client retries an operation until it
finds a stable counter value.

A single distinguished client called the co-ordinator runs any atomic increment
set operations on behalf of other clients - shown on the right hand side of
Figure~\ref{fig:counter_set}. We chose to have a single
distinguished co-ordinator to avoid making the example complex by introducting
concurrency.

The operations in Figure~\ref{fig:counter_set} do not comprehend failures.
Cyclone includes a client library to automatically sequence RPC calls at the client
side, retry them, connect to a new quorum leader on a view change and finally
take care of unrealiable networks. At the server side the results of increment,
prepare and commit RPC calls are always persisted and replicated before the
response to the client. For individual get, increment, prepare and commit
operations therefore persistence, replication and failover is transparently
handled.

The AtomicIncrementSet operation presents a special case however. Clients can
fail and therefore a client executing the steps of the operation can fail midway
leaving some counters in an unstable state forever. The solution is the
replicate the distinguished co-ordinator client too. The co-ordinator therefore
becomes both a server, receiving AtomicIncrementSet operations from clients as
well as a client, making individual Prepare and Commit calls to the quorum
leader. The implementation of the AtomicIncrementSet however requires guarantees
beyond that of simple asynchronous replication

\begin{itemize}
  \item Output Commit: AtomicIncrementSet has externally visible side effects at
    other servers when execting. It should therefore be guaranteed to finish
    executing once it starts
  \item Exactly once: Regardless of restarts AtomicIncrementSet should be
    preceived to have executed exactly once by an external observer.
\end{itemize}

The solution is to simply run AtomicIncrementSet in synchronous replication
mode. This guarantees the first property. It also guarantees that exactly once
execution will succced in ``leader'' mode with followers simply remembering the
success status of a transaction. To ensure that due to a failover we do not
prepare or commit a counter more than once, we use exactly once RPCs that
prevent RPC calls from executing more than once. Finally, since only
one AtomicIncrementSet is executed at a time, we include the current step (the
place in the for loop) in RPC calls to the quorum leader and have it reflect the
information back in the response (a cookie of sorts). By querying the results of
the last made call, the co-ordinator can simply fast forward to the correct
index in the for loop during recovery and use the value of the ``success'' field
in the response to update its own local success variable.


\begin{figure*}
\begin{tabular}{cc}    
\begin{minipage}{0.5\textwidth}  
{ \scriptsize
\begin{verbatim}
bool is_stable(uint64_t value)
{
  if(value is even) {
    return true
  }
  else {
    return false
  }
}

Get(uint64_t index)
{
  if(not is_stable(counters[index]) {
    return EUNSTABLE
  }
  else {
    return counters[index]
  }
}

Increment(uint64_t index)
{
  if(not is_stable(counters[index]) {
    return EUNSTABLE
  }
  else {
    counters[index] = counters[index] + 2
    return SUCCESS
  }
}
\end{verbatim}
}
\end{minipage} &
\begin{minipage}{0.5\textwidth}  
{ \scriptsize
\begin{verbatim}
Prepare(uint64_t index, 
        uint64_t value, 
        bool success)
{
  if(counters[index] == value) {
    success = success AND TRUE
  }
  else {
    success = success AND FALSE
  }
  // Make unstable
  counters[index] = counters[index] + 1
}

Commit(uint64_t index, bool success)
{
  if(success == TRUE) {
    // Update
    counters[index] = counters[index] + 1
  }
  else {
    // Rollback
    counters[index] = counters[index] - 1
  }
}

AtomicIncrementSet(uint64_t indices[],
                   uint64_t pre_values[],
                   int count)
{
  bool success = TRUE
  for i = 1 to count
    success =
      success AND
        Prepare(indices[i],
                pre_values[i],
                success)
  for i = 1 to count
    Commit(indices[i], success)       
  return success
}
\end{verbatim}
}
\end{minipage}
\end{tabular}
\caption{Counter Set}
\label{fig:counter_set}
\end{figure*}

\section{Related Work}

\end{document}



